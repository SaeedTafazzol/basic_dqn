{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlc-dqn-atari.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNXyqsTnYhU0XvmmNxXYaZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moodlep/basic_dqn/blob/master/mlc_dqn_atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Af9yIW6Gy5h"
      },
      "source": [
        "# Do we need this?? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sthShy9MliKv",
        "outputId": "2b54fd54-efd1-47c3-a0c4-6f3ed07ffd67"
      },
      "source": [
        "!pip install ale-py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ale-py\n",
            "  Downloading ale_py-0.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ale-py) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from ale-py) (4.8.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py) (5.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ale-py) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ale-py) (3.10.0.2)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA9qFQeVlrXD"
      },
      "source": [
        "from ale_py import ALEInterface\n",
        "\n",
        "ale = ALEInterface()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnW2Pbwnlxk_"
      },
      "source": [
        "from ale_py.roms import Breakout\n",
        "\n",
        "ale.loadROM(Breakout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QYknFN5G8F4"
      },
      "source": [
        "Below taken from [link](https://colab.research.google.com/github/GiannisMitr/DQN-Atari-Breakout/blob/master/dqn_atari_breakout.ipynb)\n",
        "\n",
        "\n",
        "# Install Atari ROMs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C5cmoBmvziM"
      },
      "source": [
        "#Make necessary imports\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st3lWelal27F",
        "outputId": "b4df39dc-478e-49c5-e570-e24524127953"
      },
      "source": [
        "# Test a basic env: \n",
        "import gym\n",
        "env = gym.make('Breakout-v4')\n",
        "env.observation_space.shape, env.action_space.n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((210, 160, 3), 18)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prFTtwJ_HWZv"
      },
      "source": [
        "## SB3 has some nice wrappers we can use, taken from cleanrl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdeJoGMrsKiR"
      },
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-o8OYWCqYek"
      },
      "source": [
        "# taken from https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/atari/dqn_atari.py\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from gym.spaces import Discrete\n",
        "from gym.wrappers import Monitor\n",
        "from stable_baselines3.common.atari_wrappers import (\n",
        "    ClipRewardEnv,\n",
        "    EpisodicLifeEnv,\n",
        "    FireResetEnv,\n",
        "    MaxAndSkipEnv,\n",
        "    NoopResetEnv,\n",
        "    WarpFrame,\n",
        ")\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgUk9dYjsImu"
      },
      "source": [
        "# taken from https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/atari/dqn_atari.py\n",
        "def make_env(gym_id, idx):\n",
        "    def thunk():\n",
        "        env = gym.make(gym_id)\n",
        "        env = NoopResetEnv(env, noop_max=30)\n",
        "        env = MaxAndSkipEnv(env, skip=4)\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        # if args.capture_video:\n",
        "        #     if idx == 0:\n",
        "        #         env = Monitor(env, f\"videos/{experiment_name}\")\n",
        "        env = EpisodicLifeEnv(env)\n",
        "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
        "            env = FireResetEnv(env)\n",
        "        env = WarpFrame(env, width=84, height=84)\n",
        "        env = ClipRewardEnv(env)\n",
        "        # env.seed(seed)\n",
        "        # env.action_space.seed(seed)\n",
        "        # env.observation_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH58jaiTspxr"
      },
      "source": [
        "envs = VecFrameStack(\n",
        "        DummyVecEnv([make_env('Breakout-v4', 0)]),\n",
        "        4,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeM1vBxCs7yL",
        "outputId": "f7d59395-3086-4c49-d62a-36c154748c6a"
      },
      "source": [
        "envs.observation_space.shape, envs.action_space.n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((84, 84, 4), 4)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9ivsrHNtBqX"
      },
      "source": [
        "envs.observation_space.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHctJStEHdrx"
      },
      "source": [
        "# Coding DQN with CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPmSoBZax2km"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ74bTOewTmc"
      },
      "source": [
        "class DQN_Network(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN_Network, self).__init__()\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        # create the dqn and target network architecture\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, 8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2), \n",
        "            nn.ReLU(), \n",
        "            nn.Conv2d(64, 64, 3, stride=1), \n",
        "            nn.ReLU(), \n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*7*7, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, self.action_dim)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        state_t = torch.FloatTensor(input).permute(2, 0, 1).unsqueeze(0)\n",
        "        print(state_t.shape, type(state_t))\n",
        "\n",
        "        q_values = self.network(state_t)\n",
        "        return q_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVWEASZSzUqQ",
        "outputId": "51308746-1a01-4b1e-ef95-91d5fc82d7e4"
      },
      "source": [
        "state = envs.observation_space.sample()\n",
        "state.shape, type(state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((84, 84, 4), numpy.ndarray)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC6hfxz4zZbC"
      },
      "source": [
        "dqn = DQN_Network(envs.observation_space.shape, envs.action_space.n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJdA1J9mzlLD",
        "outputId": "e0b284a1-6d4b-4879-d905-1d0742687b28"
      },
      "source": [
        "dqn(state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 84, 84]) <class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1129, -2.5360, -2.8048, -0.0035]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x658C1VBx3IZ"
      },
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fqz9H_6zqpk"
      },
      "source": [
        "# Taken from https://github.com/sfujim/TD3/blob/master/utils.py\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\t\tself.size = 0\n",
        "\n",
        "\t\tself.state = np.zeros((max_size, state_dim))\n",
        "\t\tself.action = np.zeros((max_size, action_dim))\n",
        "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
        "\t\tself.reward = np.zeros((max_size, 1))\n",
        "\t\tself.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\tdef add(self, state, action, next_state, reward, done):\n",
        "\t\tself.state[self.ptr] = state\n",
        "\t\tself.action[self.ptr] = action\n",
        "\t\tself.next_state[self.ptr] = next_state\n",
        "\t\tself.reward[self.ptr] = reward\n",
        "\t\tself.not_done[self.ptr] = 1. - done\n",
        "\n",
        "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\tself.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "\t\treturn (\n",
        "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "\t\t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkKamW3Qx55f"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ovTh8K0xowP"
      },
      "source": [
        "config_cnn = {\n",
        "    'env': 'Breakout-v0',\n",
        "    'total_timesteps': 300000,\n",
        "    'replay_buffer_size': 5000,\n",
        "    'minimum_replay_before_updates': 1000,\n",
        "    'target_update_steps': 1000,\n",
        "    'epsilon_start': 1.0,\n",
        "    'epsilon_end': 0.01,\n",
        "    'exploration_percentage': 0.9,\n",
        "    'hidden': 128,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.0001,\n",
        "    'gamma': 0.99,\n",
        "    'print_steps': 10000,\n",
        "    'tensorboard_folder': './tensorboard/',\n",
        "    'checkpointing_steps': 10000,\n",
        "    'biased_exploration': True,\n",
        "    'seed': 12345,\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGRG3J4Tz3ZI",
        "outputId": "e365e899-b044-4df6-8216-f3a31f5361b5"
      },
      "source": [
        "envs.action_space.sample()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_jfU9-7x7dJ"
      },
      "source": [
        "# DQN main class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeQCDmsVwupR"
      },
      "source": [
        "import time, datetime,os\n",
        "\n",
        "class DQN():\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(DQN).__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        # init the env\n",
        "        self.env = VecFrameStack(\n",
        "        DummyVecEnv([make_env(config['env'], 0)]),4)\n",
        "\n",
        "        self.action_dim = self.env.action_space.n\n",
        "        if type(self.env.observation_space) is gym.spaces.discrete.Discrete:\n",
        "            self.state_dim = self.env.observation_space.n\n",
        "        else:\n",
        "            self.state_dim = self.env.observation_space.shape[0]\n",
        "\n",
        "        # init the replay buffer\n",
        "        self.buffer = ReplayBuffer(self.state_dim, self.action_dim, self.config['replay_buffer_size'])\n",
        "\n",
        "        # Setup the DQN and Target networks and the syncing mechanism\n",
        "        self.dqn = DQN_Network(self.state_dim, self.action_dim)\n",
        "        self.target = DQN_Network(self.state_dim, self.action_dim)\n",
        "        # Sync target and dqn networks\n",
        "        self.update_target_network()\n",
        "\n",
        "        # setup the optimiser: \n",
        "        self.opt = torch.optim.Adam(self.dqn.parameters(),lr=config['learning_rate'])\n",
        "\n",
        "        # Setup the tensorboard writer\n",
        "        self.setup_logging()\n",
        "        self.epsilon_setup()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        ts = time.time()\n",
        "        self.timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "        log_dir = self.config['tensorboard_folder']+self.timestamp\n",
        "        if not os.path.exists(log_dir):\n",
        "            os.makedirs(log_dir)\n",
        "        self.summary_writer = SummaryWriter(log_dir=log_dir, flush_secs=30)\n",
        "\n",
        "    def epsilon_setup(self):\n",
        "        self.duration = self.config['total_timesteps'] * self.config['exploration_percentage']\n",
        "        self.start_e = self.config['epsilon_start']\n",
        "        self.end_e = self.config['epsilon_end']\n",
        "        self.slope =  (self.end_e - self.start_e) / self.duration\n",
        "        self.epsilon = self.start_e\n",
        "\n",
        "    def epsilon_linear_schedule(self, t: int):\n",
        "        # borrowed from cleanrl just cause it was neater...\n",
        "        return max(self.slope * t + self.start_e, self.end_e)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "          action = torch.tensor(self.env.action_space.sample(), dtype=torch.int64)\n",
        "        else:\n",
        "          with torch.no_grad():\n",
        "              action = self.dqn(state).argmax()  # n_batch (= 1) x n_actions\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        # copy network params from DQN to Target NN\n",
        "        self.target.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "    def calculate_targets(self, next_states, rewards, not_dones):\n",
        "\n",
        "        next_action_values = self.target(next_states)\n",
        "        targets = rewards + self.config['gamma'] * not_dones * (\n",
        "            torch.max(next_action_values.detach(),dim=1).values)\n",
        "\n",
        "        return targets.unsqueeze(-1)  ### Stopped here\n",
        "\n",
        "    def get_batch_from_buffer(self):\n",
        "        # sample a batch and return as tensors\n",
        "        states, next_states, actions, rewards, dones = self.buffer.sample(self.config['batch_size'])\n",
        "        states_t = torch.tensor(states, dtype=torch.float)\n",
        "        next_states_t = torch.tensor(states, dtype=torch.float)\n",
        "        actions_t = torch.tensor(actions, dtype=torch.int)\n",
        "        rewards_t = torch.tensor(rewards, dtype=torch.float)\n",
        "        dones_t = torch.tensor(dones, dtype=torch.bool)\n",
        "\n",
        "        return states_t, actions_t, next_states_t, rewards_t, dones_t\n",
        "\n",
        "    def log_network_stats(self):\n",
        "        # capture the grad norm to tensorboard\n",
        "        pass\n",
        "\n",
        "    def mse_loss(self, states, actions, targets):\n",
        "        # get q-values - forward predictions\n",
        "        current_q_values = self.dqn.forward(states)\n",
        "\n",
        "        # print(\"all q_values: \", current_q_values.detach().shape)\n",
        "        # print(\"actions: \", len(actions), actions)\n",
        "        actions_t = torch.tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
        "        q_values = torch.gather(current_q_values, -1, actions_t)\n",
        "        # MSE loss\n",
        "        loss = F.mse_loss(q_values, targets)\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "\n",
        "        self.log_network_stats()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def save(self, filename):\n",
        "        torch.save(self.dqn.state_dict(), filename+\"_\"+str(self.timestamp))\n",
        "\n",
        "    def train(self):\n",
        "        total_steps = 0\n",
        "        episode_counter = 0\n",
        "        state = self.env.reset()\n",
        "\n",
        "        ep_rewards = []\n",
        "        ep_len = 0\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # With a max step count limit, run episodes, collect data and update the DQN.\n",
        "        # Outside loop counts max steps for DQN in total. Could also set this to a max episode count\n",
        "        # When do we perform the update: once per step (not waiting for episode completion or anything)\n",
        "\n",
        "        while total_steps < self.config['total_timesteps']:\n",
        "\n",
        "            # Collect data until the buffer has reached the minimum size\n",
        "            self.epsilon = self.epsilon_linear_schedule(total_steps)\n",
        "            self.summary_writer.add_scalar(\"epsilon\", self.epsilon, total_steps)\n",
        "\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "\n",
        "            # collect data\n",
        "            self.buffer.insert_datapoint(state, action, next_state, reward, done)\n",
        "\n",
        "            if self.buffer.len() < self.config['minimum_replay_before_updates']:\n",
        "                if done:\n",
        "                    state = self.env.reset()\n",
        "                else:\n",
        "                    state = next_state\n",
        "                continue\n",
        "\n",
        "            # When do we start logging the rewards and lengths? After we have min buffer info?\n",
        "            ep_rewards.append(reward)\n",
        "            ep_len +=1\n",
        "\n",
        "            # Update the target network from the dqn's latest weights:\n",
        "            if total_steps % self.config['target_update_steps'] ==0:\n",
        "                self.update_target_network()\n",
        "\n",
        "            # If we have enough data in the replay buffer, start training a batch after every step is collected from\n",
        "            # the end, i.e. update the DQN\n",
        "            # 1. sample a batch from the buffer\n",
        "            states, next_states, actions, rewards, dones = self.buffer.sample(self.config['batch_size'])\n",
        "\n",
        "            # 2. calculate the target q-values\n",
        "            targets = self.calculate_targets(next_states, rewards, dones)\n",
        "\n",
        "            # 3. Calculate MSE loss\n",
        "            loss = self.mse_loss(states, actions, targets)  # returns loss.item()\n",
        "            running_loss += loss\n",
        "            self.summary_writer.add_scalar(\"loss\", loss, total_steps-self.config['minimum_replay_before_updates'])\n",
        "            self.summary_writer.add_scalar(\"running_loss\", running_loss, total_steps-self.config[\n",
        "                'minimum_replay_before_updates'])\n",
        "\n",
        "            # set state to next_state\n",
        "            state = next_state\n",
        "\n",
        "            # update step counter\n",
        "            total_steps+=1\n",
        "\n",
        "            # Print some stats to screen\n",
        "            if total_steps % self.config['print_steps'] == 0:\n",
        "                print(\"Loss at \", total_steps, \" steps is \", loss)\n",
        "\n",
        "            if total_steps % self.config['checkpointing_steps'] == 0:\n",
        "                self.save(\"chkpts/chkpt_dqn_\"+str(total_steps/self.config['checkpointing_steps']))\n",
        "\n",
        "            # process episode\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                self.summary_writer.add_scalar(\"episode_reward\", sum(ep_rewards), episode_counter)\n",
        "                self.summary_writer.add_scalar(\"episode_length\", ep_len, episode_counter)\n",
        "                episode_counter +=1\n",
        "                ep_rewards = []\n",
        "                ep_len = 0\n",
        "\n",
        "            self.summary_writer.flush()\n",
        "\n",
        "        self.summary_writer.close()\n",
        "        self.save(\"chkpts/final_chkpt_dqn\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}